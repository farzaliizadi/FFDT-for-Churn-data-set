# Fast-and-Frugal Decision Trees
#Fast-and-Frugal Decision Tree (Martignon et al., 2003).
rm(list=ls())
setwd("D:/Documents/RD/RD2")
library(FFTrees)
library(readxl)
# Data
df <- read.csv("Churn.csv")
dim(df)
head(df)
str(df)
names(df)
df$Churn
unique(df)
sum(is.na(df))
colSums(is.na(df))
df$Churn <- ifelse(df$Churn=='yes',1,0)
df$Churn
df <- df[, -c(20,21)] 
library(dummies)
df <- dummy.data.frame(df,names = c("State"))
# Split Dataset
set.seed(7532)
sam <- sample(2, nrow(df), replace = T, prob = c(0.80, 0.2))
train <- df[sam==1,]
test <- df[sam==2,]
# Tree Model
#c(0, 1) =  c("No", "Yes")
tree <- FFTrees(formula =Churn ~ .,
                data = train,
                data.test = test,
                main = "Customer will churn or not",
                decision.labels = c("No", "Yes"))
tree
# the value speed = mcu=Mean cue used==.... for the training data, means that the tree uses, 
#on average 1.95 pieces of information to classify cases 
# pci = % cues ignored
names(tree)
tree$formula
tree$tree.definitions
tree$data.desc
tree$cue.accuracies
tree$cost
tree$inwords
tree$tree.stats # was previously summary
tree$level.stats
tree$decision
tree$params
tree$levelout
tree$tree.max
tree$comp
tree$data
t <- cbind(train$Churn, tree$decision$train)
head(t, n=10)
s <- cbind(test$Churn, tree$decision$test)
head(s, n=10)   #0=False and 1=True are 6 tree decisions.
# Plot
plot(tree) 
plot(tree, tree = 2)       # This the best among the others
plot(tree, tree = 3)
plot(tree, tree = 4)
plot(tree, tree = 5)
plot(tree, tree = 6)
plot(tree, data = 'test')
plot(tree, data = 'test', tree=5)
plot(tree, what = 'cues')
plot(tree, stats = FALSE)
# Predict
p <- predict(tree, test)
dim(test)  
head(p)
sum(head(p))
sum(p)  #out of ... test data ... predicted TRUE.
# Cue Importance
forest <- FFForest(Churn ~ .,
                   data = df,
                   ntree = 50,
                   train.p = 0.5)
forest
plot(forest)
library(yarrr)
rm(list=ls())